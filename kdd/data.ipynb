{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"http_proxy\"] = \"http://localhost:7890\"\n",
    "os.environ[\"https_proxy\"] = \"http://localhost:7890\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/jqzhang/Miniconda3/envs/dd/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import disable_progress_bar, load_dataset, load_from_disk\n",
    "from datasets.dataset_dict import DatasetDict\n",
    "from recbole.data import create_dataset, data_preparation\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "disable_progress_bar()\n",
    "\n",
    "\n",
    "TASK_ATTRS = {\n",
    "    # Amazon-beauty\n",
    "    \"beauty\": {\n",
    "        \"load_args\": (\"McAuley-Lab/Amazon-Reviews-2023\", \"0core_rating_only_All_Beauty\"),\n",
    "        \"metric_keys\": (\"accuracy\", \"recall\"),\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataConfig:\n",
    "    task_name: str\n",
    "    datasets_path: str\n",
    "    preprocessed_datasets_path: str\n",
    "    train_batch_size: int\n",
    "    valid_batch_size: int\n",
    "    test_batch_size: int\n",
    "    num_proc: int\n",
    "    force_preprocess: bool\n",
    "    MAX_ITEM_LIST_LENGTH: int\n",
    "    load_col: dict\n",
    "    train_neg_sample_args: dict\n",
    "    train_net_sample_args: None\n",
    "\n",
    "\n",
    "class DataModule:\n",
    "    \"\"\"DataModule class\n",
    "    ```\n",
    "    data_module = DataModule(\n",
    "        config.data,\n",
    "    )\n",
    "    # preprocess datasets\n",
    "    data_module.run_preprocess(tokenizer=tokenizer)\n",
    "    # preprocess external dataset (distilled data)\n",
    "    data_module.preprocess_dataset(tokenizer=tokenizer, dataset=dataset)\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "        # load raw dataset\n",
    "        self.dataset_attr = TASK_ATTRS[self.config.task_name]\n",
    "        self.datasets: DatasetDict = self.get_dataset()\n",
    "        logger.info(f\"Datasets: {self.datasets}\")\n",
    "\n",
    "        # preprocessed_dataset\n",
    "        self.run_preprocess()\n",
    "\n",
    "        # generate dataloader\n",
    "        self.train_loader = None\n",
    "        self.valid_loader = None\n",
    "        self.get_dataloader()\n",
    "\n",
    "    def get_dataset(self):\n",
    "        \"\"\"load raw datasets from source\"\"\"\n",
    "        if os.path.exists(self.config.datasets_path):\n",
    "            datasets = load_from_disk(self.config.datasets_path)\n",
    "        else:\n",
    "            assert self.config.task_name in TASK_ATTRS\n",
    "            datasets = load_dataset(*self.dataset_attr[\"load_args\"])\n",
    "\n",
    "            # assert datasets.keys() >= {\"train\", \"valid\"}\n",
    "\n",
    "            os.makedirs(os.path.dirname(self.config.datasets_path), exist_ok=True)\n",
    "            datasets.save_to_disk(self.config.datasets_path)\n",
    "\n",
    "        return datasets\n",
    "\n",
    "    def run_preprocess(self):\n",
    "        \"\"\"datasets preprocessing\"\"\"\n",
    "\n",
    "        if (\n",
    "            os.path.exists(self.config.preprocessed_datasets_path)\n",
    "            and not self.config.force_preprocess\n",
    "        ):\n",
    "            logger.info(\n",
    "                \"Load preprocessed datasets from `{}`\".format(\n",
    "                    self.config.preprocessed_datasets_path\n",
    "                )\n",
    "            )\n",
    "            self.preprocessed_datasets = load_from_disk(\n",
    "                self.config.preprocessed_datasets_path\n",
    "            )\n",
    "            return\n",
    "\n",
    "        self.preprocessed_datasets = self.preprocess_dataset(dataset=self.datasets)\n",
    "\n",
    "        logger.info(\n",
    "            f\"Save preprocessed datasets to `{self.config.preprocessed_datasets_path}`\"\n",
    "        )\n",
    "        os.makedirs(\n",
    "            os.path.dirname(self.config.preprocessed_datasets_path), exist_ok=True\n",
    "        )\n",
    "        self.preprocessed_datasets.to_csv(\n",
    "            os.path.join(\n",
    "                self.config.preprocessed_datasets_path,\n",
    "                f\"{self.config.task_name}.inter\",\n",
    "            ),\n",
    "            sep=\"\\t\",\n",
    "            index=False,\n",
    "        )\n",
    "\n",
    "    def preprocess_dataset(self, dataset):\n",
    "        dataset_df = pd.DataFrame(dataset)\n",
    "        dataset_df.columns = [\"uid\", \"iid\", \"rating\", \"timestamp\"]\n",
    "\n",
    "        ### Filter users and items with less than 5 interactions ###\n",
    "        filtered_review_df = dataset_df.groupby(\"iid\").filter(lambda x: len(x) >= 3)\n",
    "        filtered_review_df = (\n",
    "            filtered_review_df.groupby(\"uid\")\n",
    "            .filter(lambda x: len(x) >= 5)\n",
    "            .groupby(\"uid\")\n",
    "            .apply(\n",
    "                lambda x: x.sort_values(by=[\"timestamp\"], ascending=[True]),\n",
    "                include_groups=True,\n",
    "            )\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        ### ID map ###\n",
    "        unique_uids = filtered_review_df[\"uid\"].unique()\n",
    "        unique_iids = filtered_review_df[\"iid\"].unique()\n",
    "        uid_map = {old_id: new_id for new_id, old_id in enumerate(unique_uids)}\n",
    "        iid_map = {old_id: new_id for new_id, old_id in enumerate(unique_iids)}\n",
    "        mapped_review_df = filtered_review_df.copy()\n",
    "        mapped_review_df[\"uid\"] = mapped_review_df[\"uid\"].map(uid_map)\n",
    "        mapped_review_df[\"iid\"] = mapped_review_df[\"iid\"].map(iid_map)\n",
    "        mapped_review_df.columns = [\n",
    "            \"user_id:token\",\n",
    "            \"item_id:token\",\n",
    "            \"rating:float\",\n",
    "            \"timestamp:float\",\n",
    "        ]\n",
    "\n",
    "        return mapped_review_df\n",
    "\n",
    "    def get_dataloader(self):\n",
    "        # dataset filtering\n",
    "        dataset = create_dataset(self.config)\n",
    "        # dataset splitting\n",
    "        self.train_loader, self.valid_loader, _ = data_preparation(self.config, dataset)\n",
    "\n",
    "    def train_loader(self):\n",
    "        return self.train_loader\n",
    "\n",
    "    def valid_loader(self):\n",
    "        return self.valid_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
