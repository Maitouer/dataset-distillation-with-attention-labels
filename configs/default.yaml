base:
  experiment_name: ${data.task_name}.${model.model_name}
  method: syn_seq_num_${distilled_data.syn_seq_num}.syn_seq_len_${distilled_data.syn_seq_len}.attn_label_type_${distilled_data.attention_label_type}.train_step_${learner_train.train_step}.batch_size_${learner_train.batch_size}.lr_init_${distilled_data.lr_init}.distill_lr_${train.lr_inputs_embeds}
  run_name: ${base.method}.${now:%Y-%m-%d.%H-%M-%S}
  save_dir_root: ./save
  save_method_dir: ${base.save_dir_root}/${base.experiment_name}/${base.method}
  save_dir: ${base.save_method_dir}/${now:%Y-%m-%d.%H-%M-%S}
  data_dir_root: ./data
  seed: 42
  # run_baseline: True
  pretrain: True

data:
  task_name: books
  datasets_path: ${base.data_dir_root}/${data.task_name}/datasets
  preprocessed_datasets_path: ${base.data_dir_root}/${data.task_name}/${data.task_name}.inter
  train_batch_size: 32
  valid_batch_size: 256
  test_batch_size: 256
  num_proc: 1
  force_preprocess: False
  model: SASRec
  recbole_config: ./configs/sasrec.yaml

model:
  task_name: ${data.task_name}
  model_name: SASRec
  use_pretrained_model: True
  disable_dropout: True
  n_layers: 2                     # (int) The number of transformer layers in transformer encoder.
  n_heads: 2                      # (int) The number of attention heads for multi-head attention layer.
  hidden_size: 64                 # (int) The number of features in the hidden state.
  inner_size: 256                 # (int) The inner hidden size in feed-forward layer.
  hidden_dropout_prob: 0.5        # (float) The probability of an element to be zeroed.
  attn_dropout_prob: 0.5          # (float) The probability of an attention score to be zeroed.
  hidden_act: 'gelu'              # (str) The activation function in feed-forward layer.
  layer_norm_eps: 1e-12           # (float) A value added to the denominator for numerical stability. 
  initializer_range: 0.02         # (float) The standard deviation for normal initialization.
  loss_type: 'CE'                 # (str) The type of loss function. Range in ['BPR', 'CE'].

distilled_data:
  pretrained_data_path: null
  attention_label_type: 'all' # [none, cls, all]
  attention_loss_lambda: 1.0
  syn_seq_num: 64
  syn_seq_len: 20
  lr_for_step: True
  lr_init: 1.0e-2
  lr_linear_decay: False
  fix_order: True

learner_train:
  train_step: 8
  batch_size: 8

train:
  skip_train: False
  inner_loop: ${learner_train.train_step}
  epoch: 200
  lr_inputs_embeds: 1.0e-2
  lr_attention_labels: ${train.lr_inputs_embeds}
  lr_labels: ${train.lr_inputs_embeds}
  lr_lr: ${train.lr_inputs_embeds}
  optimizer_type: adamw # [sgd, adam, adamw]
  scheduler_type: linear
  warmup_ratio: 0.1
  weight_decay: 0.0
  max_grad_norm: 1.0
  val_interval: 1
  log_interval: -1
  n_eval_model: 5
  save_ckpt_dir: ${base.save_dir}/checkpoints
  fp16: False
  bf16: False

evaluate:
  task_name: ${data.task_name}
  n_eval_model: 100  # for reset model interval
  fp16: False
  bf16: False

hydra:
  run:
    dir: ${base.save_dir}
  sweep:
    dir: ${base.save_method_dir}
    subdir: ${base.run_name}
